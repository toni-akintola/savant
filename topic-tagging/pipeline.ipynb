{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load NLP models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# === 1. Preprocessing Function ===\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and normalizes free-text bios.\"\"\"\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    \n",
    "    # NLP Processing with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenization, Stopword Removal, and Lemmatization\n",
    "    processed_tokens = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_punct]\n",
    "    \n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "# === 2. Named Entity Extraction ===\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extracts named entities like skills and organizations.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    skills, organizations = [], []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\":  # Organizations\n",
    "            organizations.append(ent.text)\n",
    "        elif ent.label_ in [\"NORP\", \"PRODUCT\", \"WORK_OF_ART\"]:  # Skills-related\n",
    "            skills.append(ent.text)\n",
    "\n",
    "    return {\"skills\": skills, \"organizations\": organizations}\n",
    "\n",
    "# === 3. Entity-Enhanced Bio Processing ===\n",
    "def preprocess_text_with_entities(text):\n",
    "    \"\"\"Preprocesses bio and appends extracted entities.\"\"\"\n",
    "    clean_text = preprocess_text(text)\n",
    "    entities = extract_entities(text)\n",
    "\n",
    "    if entities[\"skills\"]:\n",
    "        clean_text += \" [skills: \" + \", \".join(entities[\"skills\"]) + \"]\"\n",
    "    if entities[\"organizations\"]:\n",
    "        clean_text += \" [organizations: \" + \", \".join(entities[\"organizations\"]) + \"]\"\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "# === 4. Sample Data ===\n",
    "bios = {\n",
    "    \"ML Eng\": \"Dr. Jane Doe is a Machine Learning Engineer at Google, specializing in NLP & AI.\",\n",
    "    \"Cyber Analyst\": \"John Smith is a cybersecurity analyst working on network security and penetration testing.\",\n",
    "    \"Financial Analyst\": \"Alice is a financial analyst with expertise in investment banking and risk management.\",\n",
    "    \"Software Eng\": \"Bob is a software engineer focusing on backend development and distributed systems.\",\n",
    "}\n",
    "\n",
    "topics = [\"Machine Learning\", \"Artificial Intelligence\", \"Cybersecurity\", \"Finance\", \"Software Engineering\"]\n",
    "\n",
    "# === 5. Apply Preprocessing & Extract Entities ===\n",
    "processed_bios = [preprocess_text_with_entities(bios[bio]) for bio in bios]\n",
    "\n",
    "# === 6. Generate Embeddings ===\n",
    "bio_embeddings = model.encode(processed_bios, convert_to_tensor=False)\n",
    "topic_embeddings = model.encode(topics, convert_to_tensor=False)\n",
    "\n",
    "# Combine all embeddings\n",
    "all_embeddings = np.vstack([bio_embeddings, topic_embeddings])\n",
    "\n",
    "# === 7. Dimensionality Reduction ===\n",
    "dim_reduction = PCA(n_components=2)  # Switch to TSNE(n_components=2, perplexity=5) for better clustering\n",
    "reduced_embeddings = dim_reduction.fit_transform(all_embeddings)\n",
    "\n",
    "# Split back into bios and topics\n",
    "bio_reduced = reduced_embeddings[: len(bios)]\n",
    "topic_reduced = reduced_embeddings[len(bios):]\n",
    "\n",
    "# === 8. Visualization ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot bios\n",
    "plt.scatter(bio_reduced[:, 0], bio_reduced[:, 1], c=\"blue\", label=\"Bios\", alpha=0.7, edgecolors=\"k\")\n",
    "\n",
    "# Plot topics\n",
    "plt.scatter(topic_reduced[:, 0], topic_reduced[:, 1], c=\"red\", label=\"Topics\", marker=\"X\", s=100, edgecolors=\"k\")\n",
    "\n",
    "# Annotate bios\n",
    "for i, txt in enumerate(bios):\n",
    "    plt.annotate(f\"{txt}\", (bio_reduced[i, 0], bio_reduced[i, 1]), fontsize=9)\n",
    "\n",
    "# Annotate topics\n",
    "for i, txt in enumerate(topics):\n",
    "    plt.annotate(txt, (topic_reduced[i, 0], topic_reduced[i, 1]), fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.title(\"Visualization of Bio & Topic Embeddings (With Entity Enrichment)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
